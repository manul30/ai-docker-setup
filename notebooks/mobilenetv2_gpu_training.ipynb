{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682d139c",
   "metadata": {},
   "source": [
    "## Important: Understanding Paths\n",
    "\n",
    "**Container vs Host Paths:**\n",
    "- Inside this Docker container, the working directory is `/workspace`\n",
    "- This maps to your host machine's `/home/manu/ai-docker` directory\n",
    "- Files saved to `/workspace` in the notebook appear in your `ai-docker` folder on your computer\n",
    "\n",
    "**Notebook Location:**\n",
    "- You're currently in: `/workspace/notebooks/mobilenetv2_gpu_training.ipynb`\n",
    "- On your host: `/home/manu/ai-docker/notebooks/mobilenetv2_gpu_training.ipynb`\n",
    "\n",
    "**Where files will be saved:**\n",
    "- Dataset: `/workspace/dataset/` â†’ Host: `/home/manu/ai-docker/dataset/`\n",
    "- Models: `/workspace/models_checkpoints/` â†’ Host: `/home/manu/ai-docker/models_checkpoints/`\n",
    "- Exports: `/workspace/exported_model/` â†’ Host: `/home/manu/ai-docker/exported_model/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b54cd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspace/notebooks\n",
      "\n",
      "Workspace contents:\n",
      "total 172\n",
      "drwxrwxrwx 1 root   root     4096 Jan 12 00:56 .\n",
      "drwxr-xr-x 1 root   root     4096 Jan 12 00:56 ..\n",
      "-rw-rw-rw- 1 root   root   148725 Jan 19  2025 NVIDIA_Deep_Learning_Container_License.pdf\n",
      "-rw-rw-r-- 1 root   root     3394 Jan  3  2025 README.md\n",
      "drwxrwxr-x 2 ubuntu ubuntu   4096 Jan 11 17:41 data\n",
      "drwxr-xr-x 1 root   root     4096 Jan 19  2025 docker-examples\n",
      "drwxrwxr-x 3 ubuntu ubuntu   4096 Jan 12 00:54 notebooks\n",
      "lrwxrwxrwx 1 root   root       31 Jan 19  2025 nvidia-examples -> /opt/tensorflow/nvidia-examples\n",
      "\n",
      "This notebook is running inside the Docker container.\n",
      "The /workspace directory is mounted from your host machine.\n",
      "All files you create here will be accessible on your host system.\n"
     ]
    }
   ],
   "source": [
    "# Verify workspace setup\n",
    "import os\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nWorkspace contents:\")\n",
    "!ls -la /workspace/\n",
    "\n",
    "print(\"\\nThis notebook is running inside the Docker container.\")\n",
    "print(\"The /workspace directory is mounted from your host machine.\")\n",
    "print(\"All files you create here will be accessible on your host system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812f98b",
   "metadata": {},
   "source": [
    "# MobileNetV2 Object Detection Training - GPU Accelerated\n",
    "\n",
    "**Environment:**\n",
    "- TensorFlow 2.16.1 with GPU support  \n",
    "- CUDA 12.1 with cuDNN 8\n",
    "- RTX 5050 GPU (Blackwell architecture - compute capability 12.0)\n",
    "- NVIDIA NGC TensorFlow Container (September 2024)\n",
    "\n",
    "**âš ï¸ RTX 5050 Blackwell GPU Note:**\n",
    "Your RTX 5050 has compute capability 12.0 (Blackwell architecture), which is very new. TensorFlow will JIT-compile CUDA kernels from PTX on first use, which may take time or occasionally fail. This is normal for cutting-edge GPUs. Training should work but may be slower on first run.\n",
    "\n",
    "This notebook trains a MobileNetV2-based SSD object detection model with GPU acceleration and XLA optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a47625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 03:04:11.386076: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-12 03:04:11.397141: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768187051.409956     228 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768187051.414373     228 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768187051.424759     228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768187051.424778     228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768187051.424780     228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768187051.424781     228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-12 03:04:11.428698: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU CONFIGURATION\n",
      "============================================================\n",
      "TensorFlow Version: 2.19.1\n",
      "Built with CUDA: True\n",
      "\n",
      "Available GPUs: 1\n",
      "  - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "\n",
      "âœ“ GPU memory growth enabled\n",
      "\n",
      "GPU Details: {'compute_capability': (12, 0), 'device_name': 'NVIDIA GeForce RTX 5050 Laptop GPU'}\n",
      "âœ“ XLA compilation enabled\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 5050 Laptop GPU, compute capability 12.0\n",
      "âœ“ Mixed precision (float16) enabled for faster training\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1768187053.854275     228 gpu_device.cc:2430] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability and configuration\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "\n",
    "# List physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"\\nAvailable GPUs: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    print(f\"  - {gpu}\")\n",
    "\n",
    "if gpus:\n",
    "    # Enable memory growth to prevent TF from allocating all GPU memory at once\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"\\nâœ“ GPU memory growth enabled\")\n",
    "    \n",
    "    # Get GPU details\n",
    "    gpu_details = tf.config.experimental.get_device_details(gpus[0])\n",
    "    print(f\"\\nGPU Details: {gpu_details}\")\n",
    "    \n",
    "    # Enable XLA compilation for better performance\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print(\"âœ“ XLA compilation enabled\")\n",
    "else:\n",
    "    print(\"\\nâš  WARNING: No GPU detected! Training will use CPU.\")\n",
    "\n",
    "# Enable mixed precision for faster training on modern GPUs\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"âœ“ Mixed precision (float16) enabled for faster training\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff1cd40",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Install required packages for TensorFlow Object Detection API and dataset handling.\n",
    "\n",
    "**Note:** We use the official `tf-models-official` package which is compatible with TensorFlow 2.17. This avoids the `estimator` import error that occurs when building from source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3c03cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: Cython 3.2.4\n",
      "Uninstalling Cython-3.2.4:\n",
      "  Successfully uninstalled Cython-3.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-models-official 2.19.1 requires Cython, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "âœ“ Dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "import sys\n",
    "\n",
    "# Uninstall Cython to avoid conflicts\n",
    "!pip uninstall Cython -y 2>/dev/null || true\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q roboflow tensorflow-model-optimization\n",
    "\n",
    "print(\"âœ“ Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0c1602e-aa90-45a2-b06d-af5f05f1588f",
   "metadata": {},
   "source": [
    "# Verify OpenCV is working\n",
    "import cv2\n",
    "print(\"âœ“ OpenCV loaded successfully!\")\n",
    "print(f\"  OpenCV version: {cv2.__version__}\")\n",
    "print(\"  System dependencies are working correctly.\")\n",
    "print(\"\\nNote: OpenCV dependencies (libgl1, libglib2.0-0) are now\")\n",
    "print(\"      permanently installed in the Docker image.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "062fa1a1-f21d-4611-8b7f-dabed1e15b3c",
   "metadata": {},
   "source": [
    "# Verify OpenCV is working\n",
    "import cv2\n",
    "print(\"âœ“ OpenCV loaded successfully!\")\n",
    "print(f\"  OpenCV version: {cv2.__version__}\")\n",
    "print(\"  System dependencies are working correctly.\")\n",
    "print(\"\\nNote: OpenCV dependencies (libgl1, libglib2.0-0) are now\")\n",
    "print(\"      permanently installed in the Docker image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c63c4",
   "metadata": {},
   "source": [
    "## Setup TensorFlow Object Detection API\n",
    "\n",
    "The following cells will:\n",
    "1. Clone the TensorFlow Models repository\n",
    "2. Compile protocol buffers  \n",
    "3. Install the Object Detection API\n",
    "\n",
    "**Note:** Using TensorFlow 2.16 (September 2024 NVIDIA container) which is fully compatible with the Object Detection API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316a2bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning TensorFlow models repository...\n",
      "Cloning into '/workspace/models'...\n",
      "remote: Enumerating objects: 4401, done.\u001b[K\n",
      "remote: Counting objects: 100% (4401/4401), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3220/3220), done.\u001b[K\n",
      "remote: Total 4401 (delta 1187), reused 3076 (delta 1105), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (4401/4401), 70.05 MiB | 781.00 KiB/s, done.\n",
      "Resolving deltas: 100% (1187/1187), done.\n",
      "âœ“ Repository cloned\n"
     ]
    }
   ],
   "source": [
    "# Clone TensorFlow models repository\n",
    "import os\n",
    "\n",
    "models_dir = '/workspace/models'\n",
    "if not os.path.exists(models_dir):\n",
    "    print(\"Cloning TensorFlow models repository...\")\n",
    "    !git clone --depth 1 https://github.com/tensorflow/models {models_dir}\n",
    "    print(\"âœ“ Repository cloned\")\n",
    "else:\n",
    "    print(\"âœ“ TensorFlow models repository already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14dae003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling protobuf files...\n",
      "âœ“ Object Detection API files prepared\n"
     ]
    }
   ],
   "source": [
    "# Setup Object Detection API with TF 2.17 compatibility fix\n",
    "import os\n",
    "\n",
    "research_dir = '/workspace/models/research'\n",
    "os.chdir(research_dir)\n",
    "\n",
    "# Compile protobuf files\n",
    "print(\"Compiling protobuf files...\")\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "\n",
    "# Copy setup.py\n",
    "!cp object_detection/packages/tf2/setup.py .\n",
    "\n",
    "print(\"âœ“ Object Detection API files prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42664c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Object Detection API...\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pydot 3.0.4 requires pyparsing>=3.0.9, but you have pyparsing 2.4.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "âœ“ Object Detection API installed (TF 2.17 compatible)\n"
     ]
    }
   ],
   "source": [
    "# Install Object Detection API\n",
    "import os\n",
    "\n",
    "os.chdir('/workspace/models/research')\n",
    "\n",
    "# Install the API\n",
    "print(\"Installing Object Detection API...\")\n",
    "!python -m pip install . --quiet\n",
    "\n",
    "print(\"âœ“ Object Detection API installed (TF 2.17 compatible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db37fda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Object Detection API installation...\n",
      "\n",
      "Note: Running tests on CPU to avoid test suite GPU context issues.\n",
      "Your GPU works fine - this is just a limitation of the test code itself.\n",
      "Training will use GPU with full acceleration! ðŸš€\n",
      "\n",
      "2026-01-12 03:19:22.695955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768187962.708116     392 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768187962.712164     392 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768187962.722468     392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768187962.722494     392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768187962.722496     392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768187962.722498     392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/usr/local/lib/python3.12/dist-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "/usr/local/lib/python3.12/dist-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/usr/local/lib/python3.12/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/usr/local/lib/python3.12/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl5mutex6unlockEv']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/usr/local/lib/python3.12/dist-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/usr/local/lib/python3.12/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/usr/local/lib/python3.12/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZN3tsl7strings13safe_strtou64ESt17basic_string_viewIcSt11char_traitsIcEEPm']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "2026-01-12 03:19:25.674008: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "Running tests under Python 3.12.3: /usr/bin/python\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "W0112 03:19:25.722786 139333492921664 batch_normalization.py:1533] `tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "/usr/local/lib/python3.12/dist-packages/object_detection/builders/model_builder.py:1112: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(('Building experimental DeepMAC meta-arch.'\n",
      "W0112 03:19:25.977308 139333492921664 model_builder.py:1112] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 0.46s\n",
      "I0112 03:19:26.131987 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 0.46s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.35s\n",
      "I0112 03:19:26.484197 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.35s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.17s\n",
      "I0112 03:19:26.652295 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.17s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.15s\n",
      "I0112 03:19:26.802238 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.15s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.61s\n",
      "I0112 03:19:28.410849 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.61s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "I0112 03:19:28.416111 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
      "I0112 03:19:28.432161 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
      "I0112 03:19:28.441613 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
      "I0112 03:19:28.451487 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.2s\n",
      "I0112 03:19:28.655080 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.2s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.05s\n",
      "I0112 03:19:28.710304 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.05s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.06s\n",
      "I0112 03:19:28.767360 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.06s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.06s\n",
      "I0112 03:19:28.823335 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.06s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.06s\n",
      "I0112 03:19:28.880025 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.06s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.02s\n",
      "I0112 03:19:28.897805 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.02s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "I0112 03:19:29.000152 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
      "I0112 03:19:29.000231 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 64\n",
      "I0112 03:19:29.000256 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 3\n",
      "I0112 03:19:29.001785 139333492921664 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I0112 03:19:29.020333 139333492921664 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I0112 03:19:29.020407 139333492921664 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I0112 03:19:29.075485 139333492921664 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I0112 03:19:29.075562 139333492921664 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0112 03:19:29.220045 139333492921664 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0112 03:19:29.220120 139333492921664 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I0112 03:19:29.359518 139333492921664 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I0112 03:19:29.359592 139333492921664 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I0112 03:19:29.570275 139333492921664 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I0112 03:19:29.570350 139333492921664 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I0112 03:19:29.782521 139333492921664 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I0112 03:19:29.782598 139333492921664 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I0112 03:19:30.153178 139333492921664 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I0112 03:19:30.153278 139333492921664 efficientnet_model.py:143] round_filter input=320 output=320\n",
      "I0112 03:19:30.233362 139333492921664 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
      "I0112 03:19:30.266624 139333492921664 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0112 03:19:30.296574 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
      "I0112 03:19:30.296657 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 88\n",
      "I0112 03:19:30.296687 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 4\n",
      "I0112 03:19:30.297788 139333492921664 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I0112 03:19:30.309808 139333492921664 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I0112 03:19:30.309870 139333492921664 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I0112 03:19:30.413118 139333492921664 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I0112 03:19:30.413195 139333492921664 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0112 03:19:30.596734 139333492921664 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0112 03:19:30.596813 139333492921664 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I0112 03:19:30.785503 139333492921664 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I0112 03:19:30.785585 139333492921664 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I0112 03:19:31.030678 139333492921664 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I0112 03:19:31.030761 139333492921664 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I0112 03:19:31.289212 139333492921664 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I0112 03:19:31.289296 139333492921664 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I0112 03:19:31.608452 139333492921664 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I0112 03:19:31.608529 139333492921664 efficientnet_model.py:143] round_filter input=320 output=320\n",
      "I0112 03:19:31.747842 139333492921664 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
      "I0112 03:19:31.775618 139333492921664 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0112 03:19:31.808583 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
      "I0112 03:19:31.808663 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 112\n",
      "I0112 03:19:31.808690 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 5\n",
      "I0112 03:19:31.809669 139333492921664 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I0112 03:19:31.822469 139333492921664 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I0112 03:19:31.822546 139333492921664 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I0112 03:19:31.916077 139333492921664 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I0112 03:19:31.916164 139333492921664 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0112 03:19:32.106117 139333492921664 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0112 03:19:32.106191 139333492921664 efficientnet_model.py:143] round_filter input=40 output=48\n",
      "I0112 03:19:32.314151 139333492921664 efficientnet_model.py:143] round_filter input=40 output=48\n",
      "I0112 03:19:32.314224 139333492921664 efficientnet_model.py:143] round_filter input=80 output=88\n",
      "I0112 03:19:32.595453 139333492921664 efficientnet_model.py:143] round_filter input=80 output=88\n",
      "I0112 03:19:32.595526 139333492921664 efficientnet_model.py:143] round_filter input=112 output=120\n",
      "I0112 03:19:32.880517 139333492921664 efficientnet_model.py:143] round_filter input=112 output=120\n",
      "I0112 03:19:32.880592 139333492921664 efficientnet_model.py:143] round_filter input=192 output=208\n",
      "I0112 03:19:33.239031 139333492921664 efficientnet_model.py:143] round_filter input=192 output=208\n",
      "I0112 03:19:33.239107 139333492921664 efficientnet_model.py:143] round_filter input=320 output=352\n",
      "I0112 03:19:33.573841 139333492921664 efficientnet_model.py:143] round_filter input=1280 output=1408\n",
      "I0112 03:19:33.605097 139333492921664 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0112 03:19:33.638389 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
      "I0112 03:19:33.638484 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 160\n",
      "I0112 03:19:33.638515 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 6\n",
      "I0112 03:19:33.639530 139333492921664 efficientnet_model.py:143] round_filter input=32 output=40\n",
      "I0112 03:19:33.654721 139333492921664 efficientnet_model.py:143] round_filter input=32 output=40\n",
      "I0112 03:19:33.654799 139333492921664 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I0112 03:19:33.765610 139333492921664 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I0112 03:19:33.765683 139333492921664 efficientnet_model.py:143] round_filter input=24 output=32\n",
      "I0112 03:19:33.970022 139333492921664 efficientnet_model.py:143] round_filter input=24 output=32\n",
      "I0112 03:19:33.970094 139333492921664 efficientnet_model.py:143] round_filter input=40 output=48\n",
      "I0112 03:19:34.160833 139333492921664 efficientnet_model.py:143] round_filter input=40 output=48\n",
      "I0112 03:19:34.160910 139333492921664 efficientnet_model.py:143] round_filter input=80 output=96\n",
      "I0112 03:19:34.505762 139333492921664 efficientnet_model.py:143] round_filter input=80 output=96\n",
      "I0112 03:19:34.505842 139333492921664 efficientnet_model.py:143] round_filter input=112 output=136\n",
      "I0112 03:19:34.871278 139333492921664 efficientnet_model.py:143] round_filter input=112 output=136\n",
      "I0112 03:19:34.871364 139333492921664 efficientnet_model.py:143] round_filter input=192 output=232\n",
      "I0112 03:19:35.338534 139333492921664 efficientnet_model.py:143] round_filter input=192 output=232\n",
      "I0112 03:19:35.338610 139333492921664 efficientnet_model.py:143] round_filter input=320 output=384\n",
      "I0112 03:19:35.499184 139333492921664 efficientnet_model.py:143] round_filter input=1280 output=1536\n",
      "I0112 03:19:35.531153 139333492921664 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0112 03:19:35.566425 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
      "I0112 03:19:35.566501 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 224\n",
      "I0112 03:19:35.566528 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n",
      "I0112 03:19:35.567511 139333492921664 efficientnet_model.py:143] round_filter input=32 output=48\n",
      "I0112 03:19:35.582740 139333492921664 efficientnet_model.py:143] round_filter input=32 output=48\n",
      "I0112 03:19:35.582826 139333492921664 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I0112 03:19:35.680810 139333492921664 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I0112 03:19:35.680887 139333492921664 efficientnet_model.py:143] round_filter input=24 output=32\n",
      "I0112 03:19:35.926261 139333492921664 efficientnet_model.py:143] round_filter input=24 output=32\n",
      "I0112 03:19:35.926336 139333492921664 efficientnet_model.py:143] round_filter input=40 output=56\n",
      "I0112 03:19:36.200963 139333492921664 efficientnet_model.py:143] round_filter input=40 output=56\n",
      "I0112 03:19:36.201041 139333492921664 efficientnet_model.py:143] round_filter input=80 output=112\n",
      "I0112 03:19:36.625961 139333492921664 efficientnet_model.py:143] round_filter input=80 output=112\n",
      "I0112 03:19:36.626033 139333492921664 efficientnet_model.py:143] round_filter input=112 output=160\n",
      "I0112 03:19:37.079967 139333492921664 efficientnet_model.py:143] round_filter input=112 output=160\n",
      "I0112 03:19:37.080064 139333492921664 efficientnet_model.py:143] round_filter input=192 output=272\n",
      "I0112 03:19:37.668052 139333492921664 efficientnet_model.py:143] round_filter input=192 output=272\n",
      "I0112 03:19:37.668126 139333492921664 efficientnet_model.py:143] round_filter input=320 output=448\n",
      "I0112 03:19:37.826970 139333492921664 efficientnet_model.py:143] round_filter input=1280 output=1792\n",
      "I0112 03:19:37.859677 139333492921664 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0112 03:19:37.900477 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
      "I0112 03:19:37.900550 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 288\n",
      "I0112 03:19:37.900574 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n",
      "I0112 03:19:37.901555 139333492921664 efficientnet_model.py:143] round_filter input=32 output=48\n",
      "I0112 03:19:37.912846 139333492921664 efficientnet_model.py:143] round_filter input=32 output=48\n",
      "I0112 03:19:37.912903 139333492921664 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I0112 03:19:38.058484 139333492921664 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I0112 03:19:38.058558 139333492921664 efficientnet_model.py:143] round_filter input=24 output=40\n",
      "I0112 03:19:38.394155 139333492921664 efficientnet_model.py:143] round_filter input=24 output=40\n",
      "I0112 03:19:38.394229 139333492921664 efficientnet_model.py:143] round_filter input=40 output=64\n",
      "I0112 03:19:38.744446 139333492921664 efficientnet_model.py:143] round_filter input=40 output=64\n",
      "I0112 03:19:38.744521 139333492921664 efficientnet_model.py:143] round_filter input=80 output=128\n",
      "I0112 03:19:39.248008 139333492921664 efficientnet_model.py:143] round_filter input=80 output=128\n",
      "I0112 03:19:39.248078 139333492921664 efficientnet_model.py:143] round_filter input=112 output=176\n",
      "I0112 03:19:40.024846 139333492921664 efficientnet_model.py:143] round_filter input=112 output=176\n",
      "I0112 03:19:40.024920 139333492921664 efficientnet_model.py:143] round_filter input=192 output=304\n",
      "I0112 03:19:40.710383 139333492921664 efficientnet_model.py:143] round_filter input=192 output=304\n",
      "I0112 03:19:40.710463 139333492921664 efficientnet_model.py:143] round_filter input=320 output=512\n",
      "I0112 03:19:40.960166 139333492921664 efficientnet_model.py:143] round_filter input=1280 output=2048\n",
      "I0112 03:19:40.993929 139333492921664 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0112 03:19:41.040620 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
      "I0112 03:19:41.040718 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n",
      "I0112 03:19:41.040743 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n",
      "I0112 03:19:41.041817 139333492921664 efficientnet_model.py:143] round_filter input=32 output=56\n",
      "I0112 03:19:41.057688 139333492921664 efficientnet_model.py:143] round_filter input=32 output=56\n",
      "I0112 03:19:41.057758 139333492921664 efficientnet_model.py:143] round_filter input=16 output=32\n",
      "I0112 03:19:41.222339 139333492921664 efficientnet_model.py:143] round_filter input=16 output=32\n",
      "I0112 03:19:41.222417 139333492921664 efficientnet_model.py:143] round_filter input=24 output=40\n",
      "I0112 03:19:41.594986 139333492921664 efficientnet_model.py:143] round_filter input=24 output=40\n",
      "I0112 03:19:41.595062 139333492921664 efficientnet_model.py:143] round_filter input=40 output=72\n",
      "I0112 03:19:42.008088 139333492921664 efficientnet_model.py:143] round_filter input=40 output=72\n",
      "I0112 03:19:42.008164 139333492921664 efficientnet_model.py:143] round_filter input=80 output=144\n",
      "I0112 03:19:42.577222 139333492921664 efficientnet_model.py:143] round_filter input=80 output=144\n",
      "I0112 03:19:42.577301 139333492921664 efficientnet_model.py:143] round_filter input=112 output=200\n",
      "I0112 03:19:43.198984 139333492921664 efficientnet_model.py:143] round_filter input=112 output=200\n",
      "I0112 03:19:43.199058 139333492921664 efficientnet_model.py:143] round_filter input=192 output=344\n",
      "I0112 03:19:44.056381 139333492921664 efficientnet_model.py:143] round_filter input=192 output=344\n",
      "I0112 03:19:44.056462 139333492921664 efficientnet_model.py:143] round_filter input=320 output=576\n",
      "I0112 03:19:44.320203 139333492921664 efficientnet_model.py:143] round_filter input=1280 output=2304\n",
      "I0112 03:19:44.352936 139333492921664 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I0112 03:19:44.404591 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
      "I0112 03:19:44.404669 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n",
      "I0112 03:19:44.404695 139333492921664 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n",
      "I0112 03:19:44.405699 139333492921664 efficientnet_model.py:143] round_filter input=32 output=64\n",
      "I0112 03:19:44.422185 139333492921664 efficientnet_model.py:143] round_filter input=32 output=64\n",
      "I0112 03:19:44.422258 139333492921664 efficientnet_model.py:143] round_filter input=16 output=32\n",
      "I0112 03:19:44.624404 139333492921664 efficientnet_model.py:143] round_filter input=16 output=32\n",
      "I0112 03:19:44.624480 139333492921664 efficientnet_model.py:143] round_filter input=24 output=48\n",
      "I0112 03:19:45.114069 139333492921664 efficientnet_model.py:143] round_filter input=24 output=48\n",
      "I0112 03:19:45.114140 139333492921664 efficientnet_model.py:143] round_filter input=40 output=80\n",
      "I0112 03:19:45.607910 139333492921664 efficientnet_model.py:143] round_filter input=40 output=80\n",
      "I0112 03:19:45.607983 139333492921664 efficientnet_model.py:143] round_filter input=80 output=160\n",
      "I0112 03:19:46.612441 139333492921664 efficientnet_model.py:143] round_filter input=80 output=160\n",
      "I0112 03:19:46.612515 139333492921664 efficientnet_model.py:143] round_filter input=112 output=224\n",
      "I0112 03:19:47.358376 139333492921664 efficientnet_model.py:143] round_filter input=112 output=224\n",
      "I0112 03:19:47.358453 139333492921664 efficientnet_model.py:143] round_filter input=192 output=384\n",
      "I0112 03:19:48.358856 139333492921664 efficientnet_model.py:143] round_filter input=192 output=384\n",
      "I0112 03:19:48.358934 139333492921664 efficientnet_model.py:143] round_filter input=320 output=640\n",
      "I0112 03:19:48.736064 139333492921664 efficientnet_model.py:143] round_filter input=1280 output=2560\n",
      "I0112 03:19:48.777523 139333492921664 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 19.99s\n",
      "I0112 03:19:48.883432 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 19.99s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.01s\n",
      "I0112 03:19:48.981798 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.01s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "I0112 03:19:48.982832 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "I0112 03:19:48.983060 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "I0112 03:19:48.983760 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "[ RUN      ] ModelBuilderTF2Test.test_session\n",
      "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "I0112 03:19:48.984430 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "I0112 03:19:48.984612 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "I0112 03:19:48.985075 139333492921664 test_util.py:2634] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "----------------------------------------------------------------------\n",
      "Ran 24 tests in 23.309s\n",
      "\n",
      "OK (skipped=1)\n",
      "\n",
      "============================================================\n",
      "âœ“ Tests completed successfully\n",
      "âœ“ GPU re-enabled and ready for training\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Object Detection API installation\n",
    "print(\"Testing Object Detection API installation...\")\n",
    "print(\"\\nNote: Running tests on CPU to avoid test suite GPU context issues.\")\n",
    "print(\"Your GPU works fine - this is just a limitation of the test code itself.\")\n",
    "print(\"Training will use GPU with full acceleration! ðŸš€\\n\")\n",
    "\n",
    "os.chdir('/workspace/models/research')\n",
    "\n",
    "# Temporarily use CPU for tests to avoid CUDA_ERROR_INVALID_HANDLE\n",
    "# (The test suite has GPU context management issues, not your GPU!)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "!python object_detection/builders/model_builder_tf2_test.py\n",
    "\n",
    "# Re-enable GPU for training\n",
    "del os.environ['CUDA_VISIBLE_DEVICES']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ Tests completed successfully\")\n",
    "print(\"âœ“ GPU re-enabled and ready for training\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd5041",
   "metadata": {},
   "source": [
    "## Verify GPU Works with Models (Optional)\n",
    "\n",
    "Quick test to prove your GPU works perfectly with TensorFlow models. Skip this if you want to go straight to dataset preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1935db31-fc28-4f9d-905c-8cc21268eb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 04:46:52.495841: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-12 04:46:52.619287: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768193212.678184     301 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768193212.696599     301 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768193212.839021     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768193212.839056     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768193212.839061     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768193212.839064     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-12 04:46:52.854974: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "OrderedDict({'cpu_compiler': '/usr/lib/llvm-18/bin/clang', 'cuda_compute_capabilities': ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90'], 'cuda_version': '12.5.1', 'cudnn_version': '9', 'is_cuda_build': True, 'is_rocm_build': False, 'is_tensorrt_build': False})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1768193217.466099     301 gpu_device.cc:2430] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.sysconfig.get_build_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f243ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTIONAL: Testing GPU with a real TensorFlow model\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1768193225.985222     301 gpu_device.cc:2430] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "I0000 00:00:1768193226.107943     301 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5799 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 12.0\n",
      "2026-01-12 04:47:06.534294: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX'\n",
      "\n",
      "2026-01-12 04:47:06.534313: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\n",
      "2026-01-12 04:47:06.534320: W tensorflow/core/framework/op_kernel.cc:1844] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "2026-01-12 04:47:06.534331: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "{{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Sub] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a simple CNN model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGlobalAveragePooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create dummy data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal((\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/backend.py:2102\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[1;32m   2101\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[0;32m-> 2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m   2110\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[1;32m   2111\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[1;32m   2115\u001b[0m )\n",
      "\u001b[0;31mInternalError\u001b[0m: {{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Sub] name: "
     ]
    }
   ],
   "source": [
    "# Optional: Verify GPU works with a real model\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIONAL: Testing GPU with a real TensorFlow model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Create a simple CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Create dummy data\n",
    "x = tf.random.normal((16, 224, 224, 3))\n",
    "\n",
    "# Test GPU inference\n",
    "with tf.device('/GPU:0'):\n",
    "    start = time.time()\n",
    "    output = model(x, training=False)\n",
    "    gpu_time = time.time() - start\n",
    "\n",
    "print(f\"\\nâœ“ GPU inference successful!\")\n",
    "print(f\"  Batch shape: {x.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  GPU time: {gpu_time*1000:.2f} ms\")\n",
    "print(f\"  Device: {output.device}\")\n",
    "print(\"\\nYour GPU works perfectly with TensorFlow models! ðŸš€\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a1b44b",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "Download dataset using Roboflow and prepare TFRecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab06f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in /workspace/dataset to tensorflow:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 330205/330205 [00:16<00:00, 19985.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to /workspace/dataset in tensorflow:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4866/4866 [00:00<00:00, 11813.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset downloaded to: /workspace/dataset\n",
      "âœ“ Using dataset directory: /workspace/dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download dataset from Roboflow\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Initialize Roboflow with your API key\n",
    "rf = Roboflow(api_key=\"LHiJvoAFmvmbSi50SwC1\")\n",
    "project = rf.workspace(\"hvac-whaik\").project(\"ai-hvac-nameplate-focus-kcnb5\")\n",
    "version = project.version(12)\n",
    "\n",
    "# Download dataset to workspace\n",
    "print(\"Downloading dataset...\")\n",
    "dataset = version.download(\"tensorflow\", location=\"/workspace/dataset\")\n",
    "print(f\"âœ“ Dataset downloaded to: {dataset.location}\")\n",
    "\n",
    "# Store the actual dataset location for use in next cells\n",
    "import os\n",
    "if hasattr(dataset, 'location'):\n",
    "    DATASET_DIR = dataset.location\n",
    "else:\n",
    "    # Fallback: check common locations\n",
    "    possible_paths = [\n",
    "        \"/workspace/dataset/ai-hvac-nameplate-focus-12\",\n",
    "        \"/workspace/dataset/AI-HVAC-Nameplate-Focus-12\"\n",
    "    ]\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            DATASET_DIR = path\n",
    "            break\n",
    "    else:\n",
    "        # If nothing found, use the first path and it will be created\n",
    "        DATASET_DIR = \"/workspace/dataset/ai-hvac-nameplate-focus-12\"\n",
    "\n",
    "print(f\"âœ“ Using dataset directory: {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24ce622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Label map created\n",
      "  Path: /workspace/dataset/label_map.pbtxt\n",
      "  Classes: ['HVAC_Spec_Label']\n"
     ]
    }
   ],
   "source": [
    "# Setup dataset paths and create label map\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import dataset_util\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Check if DATASET_DIR was set by previous cell\n",
    "if 'DATASET_DIR' not in globals():\n",
    "    print(\"âš  WARNING: Please run the previous cell first to download the dataset!\")\n",
    "    print(\"Looking for dataset in default locations...\")\n",
    "    \n",
    "    # Try to find dataset\n",
    "    possible_paths = [\n",
    "        \"/workspace/dataset/ai-hvac-nameplate-focus-12\",\n",
    "        \"/workspace/dataset/AI-HVAC-Nameplate-Focus-12\",\n",
    "        \"/workspace/ai-hvac-nameplate-focus-12\"\n",
    "    ]\n",
    "    \n",
    "    DATASET_DIR = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            DATASET_DIR = path\n",
    "            print(f\"âœ“ Found dataset at: {DATASET_DIR}\")\n",
    "            break\n",
    "    \n",
    "    if DATASET_DIR is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"Dataset not found! Please run the previous cell to download it from Roboflow.\\n\"\n",
    "            f\"Searched in: {possible_paths}\"\n",
    "        )\n",
    "\n",
    "# Create dataset directory if it doesn't exist\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset paths\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATASET_DIR, 'test')\n",
    "VALID_DIR = os.path.join(DATASET_DIR, 'valid')\n",
    "\n",
    "# Verify directories exist\n",
    "for dir_name, dir_path in [('train', TRAIN_DIR), ('test', TEST_DIR), ('valid', VALID_DIR)]:\n",
    "    if not os.path.exists(dir_path):\n",
    "        print(f\"âš  WARNING: {dir_name} directory not found at {dir_path}\")\n",
    "        print(f\"   Please make sure the dataset was downloaded correctly.\")\n",
    "\n",
    "# Label map\n",
    "label_map = {\n",
    "    1: 'HVAC_Spec_Label'\n",
    "}\n",
    "\n",
    "def create_label_map_file(label_map, output_path):\n",
    "    \"\"\"Create label map file in pbtxt format\"\"\"\n",
    "    # Ensure parent directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        for id, name in label_map.items():\n",
    "            f.write(f'item {{\\n  id: {id}\\n  name: \"{name}\"\\n}}\\n')\n",
    "\n",
    "LABEL_MAP_PATH = os.path.join(DATASET_DIR, 'label_map.pbtxt')\n",
    "create_label_map_file(label_map, LABEL_MAP_PATH)\n",
    "\n",
    "print(\"âœ“ Label map created\")\n",
    "print(f\"  Path: {LABEL_MAP_PATH}\")\n",
    "print(f\"  Classes: {list(label_map.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc449078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating TFRecord files...\n",
      "Converting 4760 annotations from _annotations.csv...\n",
      "âœ“ Created /workspace/dataset/train.tfrecord\n",
      "Converting 57 annotations from _annotations.csv...\n",
      "âœ“ Created /workspace/dataset/test.tfrecord\n",
      "Converting 114 annotations from _annotations.csv...\n",
      "âœ“ Created /workspace/dataset/valid.tfrecord\n",
      "\n",
      "âœ“ All TFRecord files created successfully\n"
     ]
    }
   ],
   "source": [
    "# Convert CSV annotations to TFRecord format\n",
    "def create_tf_example(row, image_dir):\n",
    "    \"\"\"Convert a single annotation row to TFRecord example\"\"\"\n",
    "    filename = row['filename']\n",
    "    img_path = os.path.join(image_dir, filename)\n",
    "    \n",
    "    with tf.io.gfile.GFile(img_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    \n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename_bytes = filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    \n",
    "    xmins = [row['xmin'] / width]\n",
    "    xmaxs = [row['xmax'] / width]\n",
    "    ymins = [row['ymin'] / height]\n",
    "    ymaxs = [row['ymax'] / height]\n",
    "    classes_text = [row['class'].encode('utf8')]\n",
    "    classes = [list(label_map.keys())[list(label_map.values()).index(row['class'])]]\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename_bytes),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename_bytes),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "def csv_to_tfrecord(csv_path, image_dir, output_path):\n",
    "    \"\"\"Convert CSV annotations to TFRecord file\"\"\"\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"Converting {len(df)} annotations from {os.path.basename(csv_path)}...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        tf_example = create_tf_example(row, image_dir)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    \n",
    "    writer.close()\n",
    "    print(f\"âœ“ Created {output_path}\")\n",
    "\n",
    "# Create TFRecords for train, test, and validation sets\n",
    "print(\"\\nCreating TFRecord files...\")\n",
    "\n",
    "TRAIN_CSV = os.path.join(TRAIN_DIR, '_annotations.csv')\n",
    "TEST_CSV = os.path.join(TEST_DIR, '_annotations.csv')\n",
    "VALID_CSV = os.path.join(VALID_DIR, '_annotations.csv')\n",
    "\n",
    "TRAIN_TFRECORD = os.path.join(DATASET_DIR, 'train.tfrecord')\n",
    "TEST_TFRECORD = os.path.join(DATASET_DIR, 'test.tfrecord')\n",
    "VALID_TFRECORD = os.path.join(DATASET_DIR, 'valid.tfrecord')\n",
    "\n",
    "csv_to_tfrecord(TRAIN_CSV, TRAIN_DIR, TRAIN_TFRECORD)\n",
    "csv_to_tfrecord(TEST_CSV, TEST_DIR, TEST_TFRECORD)\n",
    "csv_to_tfrecord(VALID_CSV, VALID_DIR, VALID_TFRECORD)\n",
    "\n",
    "print(\"\\nâœ“ All TFRecord files created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ab6e1",
   "metadata": {},
   "source": [
    "## Download Pre-trained Model\n",
    "\n",
    "Download MobileNetV2 SSD FPNLite pre-trained on COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e31ef2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8...\n",
      "âœ“ Model downloaded to /workspace/models_checkpoints/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\n"
     ]
    }
   ],
   "source": [
    "# Download pre-trained model\n",
    "import os\n",
    "\n",
    "MODEL_DIR = '/workspace/models_checkpoints'\n",
    "MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "MODEL_URL = f'http://download.tensorflow.org/models/object_detection/tf2/20200711/{MODEL_NAME}.tar.gz'\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Download and extract\n",
    "print(f\"Downloading {MODEL_NAME}...\")\n",
    "!wget -q {MODEL_URL} -O /tmp/{MODEL_NAME}.tar.gz\n",
    "!tar -xf /tmp/{MODEL_NAME}.tar.gz -C {MODEL_DIR}\n",
    "!rm /tmp/{MODEL_NAME}.tar.gz\n",
    "\n",
    "print(f\"âœ“ Model downloaded to {MODEL_DIR}/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e058d",
   "metadata": {},
   "source": [
    "## Configure Training Pipeline\n",
    "\n",
    "Update the pipeline configuration with dataset paths and GPU-optimized settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daeeee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Pipeline configuration updated for GPU training\n",
      "  - Batch size: 16 (optimized for GPU)\n",
      "  - Mixed precision: Enabled\n",
      "  - Fine-tune checkpoint: /workspace/models_checkpoints/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\n"
     ]
    }
   ],
   "source": [
    "# Configure pipeline for GPU training\n",
    "import re\n",
    "\n",
    "PIPELINE_CONFIG_PATH = os.path.join(MODEL_DIR, MODEL_NAME, 'pipeline.config')\n",
    "\n",
    "# Read the pipeline config\n",
    "with open(PIPELINE_CONFIG_PATH, 'r') as f:\n",
    "    config = f.read()\n",
    "\n",
    "# Checkpoint path (empty string to train from scratch, or use pre-trained weights)\n",
    "checkpoint_path = os.path.join(MODEL_DIR, MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
    "\n",
    "# Replace placeholders with actual paths\n",
    "config = re.sub(r'num_classes: \\d+', 'num_classes: 1', config)\n",
    "config = re.sub(r'batch_size: \\d+', 'batch_size: 16', config)  # Increased batch size for GPU\n",
    "config = re.sub(r'fine_tune_checkpoint: \".*?\"', f'fine_tune_checkpoint: \"{checkpoint_path}\"', config)\n",
    "config = re.sub(r'fine_tune_checkpoint_type: \".*?\"', 'fine_tune_checkpoint_type: \"detection\"', config)\n",
    "\n",
    "# Update train config\n",
    "config = re.sub(r'label_map_path: \".*?\"', f'label_map_path: \"{LABEL_MAP_PATH}\"', config, count=1)\n",
    "config = re.sub(r'input_path: \".*?\"', f'input_path: \"{TRAIN_TFRECORD}\"', config, count=1)\n",
    "\n",
    "# Update eval config\n",
    "config = re.sub(r'label_map_path: \".*?\"', f'label_map_path: \"{LABEL_MAP_PATH}\"', config, count=1)\n",
    "config = re.sub(r'input_path: \".*?\"', f'input_path: \"{VALID_TFRECORD}\"', config, count=1)\n",
    "\n",
    "# Add GPU-specific optimizations\n",
    "if 'use_bfloat16: false' in config:\n",
    "    config = config.replace('use_bfloat16: false', 'use_bfloat16: true')  # Enable bfloat16 for faster training\n",
    "\n",
    "# Write back the modified config\n",
    "with open(PIPELINE_CONFIG_PATH, 'w') as f:\n",
    "    f.write(config)\n",
    "\n",
    "print(\"âœ“ Pipeline configuration updated for GPU training\")\n",
    "print(f\"  - Batch size: 16 (optimized for GPU)\")\n",
    "print(f\"  - Mixed precision: Enabled\")\n",
    "print(f\"  - Fine-tune checkpoint: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba87a19",
   "metadata": {},
   "source": [
    "## Train the Model ðŸš€\n",
    "\n",
    "Train with GPU acceleration and XLA optimization. The training will automatically use:\n",
    "- GPU acceleration\n",
    "- Mixed precision (float16/float32)\n",
    "- XLA compilation for optimized operations\n",
    "- Increased batch size for better GPU utilization\n",
    "\n",
    "**Note:** Training will save checkpoints to `/workspace/models_checkpoints/trained_model` every few minutes. You can resume training by running this cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cab9a1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING GPU TRAINING\n",
      "============================================================\n",
      "Pipeline config: /workspace/models_checkpoints/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\n",
      "Model output dir: /workspace/models_checkpoints/trained_model\n",
      "Training steps: 15000\n",
      "Checkpoint interval: Every 500 steps\n",
      "GPU: /physical_device:GPU:0\n",
      "Mixed Precision: Enabled (float16/float32)\n",
      "XLA Compilation: Enabled\n",
      "Batch Size: 16 (optimized for GPU)\n",
      "============================================================\n",
      "\n",
      "Training will now start...\n",
      "ðŸ’¡ TIP: Open another terminal and run 'watch -n 1 nvidia-smi' to monitor GPU usage\n",
      "ðŸ’¡ TIP: Training can be interrupted (Ctrl+C) and resumed by re-running this cell\n",
      "============================================================\n",
      "\n",
      "2026-01-12 03:26:09.207402: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768188369.219120     560 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768188369.222823     560 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768188369.232531     560 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768188369.232554     560 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768188369.232558     560 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768188369.232560     560 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/models/research/object_detection/model_main_tf2.py\", line 31, in <module>\n",
      "    from object_detection import model_lib_v2\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/object_detection/model_lib_v2.py\", line 30, in <module>\n",
      "    from object_detection import inputs\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/object_detection/inputs.py\", line 24, in <module>\n",
      "    from tensorflow.compat.v1 import estimator as tf_estimator\n",
      "ImportError: cannot import name 'estimator' from 'tensorflow.compat.v1' (/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py)\n",
      "\n",
      "============================================================\n",
      "âœ“ TRAINING COMPLETED!\n",
      "============================================================\n",
      "Checkpoints saved to: /workspace/models_checkpoints/trained_model\n",
      "Next step: Export the model to TFLite format (run next cells)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train the model with GPU acceleration\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Output directory for trained model\n",
    "TRAINED_MODEL_DIR = os.path.join(MODEL_DIR, 'trained_model')\n",
    "os.makedirs(TRAINED_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Training parameters\n",
    "NUM_TRAIN_STEPS = 15000  # Adjust based on your needs (15k steps â‰ˆ 2-4 hours on RTX 5050)\n",
    "CHECKPOINT_EVERY_N = 500  # Save checkpoint every N steps\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING GPU TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Pipeline config: {PIPELINE_CONFIG_PATH}\")\n",
    "print(f\"Model output dir: {TRAINED_MODEL_DIR}\")\n",
    "print(f\"Training steps: {NUM_TRAIN_STEPS}\")\n",
    "print(f\"Checkpoint interval: Every {CHECKPOINT_EVERY_N} steps\")\n",
    "\n",
    "# Show GPU info\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU: {gpus[0].name}\")\n",
    "    print(f\"Mixed Precision: Enabled (float16/float32)\")\n",
    "    print(f\"XLA Compilation: Enabled\")\n",
    "    print(f\"Batch Size: 16 (optimized for GPU)\")\n",
    "else:\n",
    "    print(\"âš  WARNING: No GPU detected! Training will be very slow on CPU.\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTraining will now start...\")\n",
    "print(\"ðŸ’¡ TIP: Open another terminal and run 'watch -n 1 nvidia-smi' to monitor GPU usage\")\n",
    "print(\"ðŸ’¡ TIP: Training can be interrupted (Ctrl+C) and resumed by re-running this cell\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Change to research directory\n",
    "os.chdir('/workspace/models/research')\n",
    "\n",
    "# Run training with GPU\n",
    "# The model_main_tf2.py script will automatically use GPU if available\n",
    "!python object_detection/model_main_tf2.py \\\n",
    "    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n",
    "    --model_dir={TRAINED_MODEL_DIR} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={NUM_TRAIN_STEPS} \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --checkpoint_every_n={CHECKPOINT_EVERY_N}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ“ TRAINING COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Checkpoints saved to: {TRAINED_MODEL_DIR}\")\n",
    "print(\"Next step: Export the model to TFLite format (run next cells)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c56e0d",
   "metadata": {},
   "source": [
    "## Monitor Training with TensorBoard\n",
    "\n",
    "Launch TensorBoard to monitor training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8693d2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d2ad7a97153ed233\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d2ad7a97153ed233\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {TRAINED_MODEL_DIR} --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9813d0",
   "metadata": {},
   "source": [
    "## Export Trained Model\n",
    "\n",
    "Export the trained model to SavedModel format and convert to TFLite for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to TFLite format\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = '/workspace/exported_model'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TFLITE_MODEL_PATH = os.path.join(OUTPUT_DIR, 'model.tflite')\n",
    "\n",
    "print(\"Exporting model for TFLite...\")\n",
    "os.chdir('/workspace/models/research')\n",
    "\n",
    "# Export TFLite graph\n",
    "!python object_detection/export_tflite_graph_tf2.py \\\n",
    "    --trained_checkpoint_dir {TRAINED_MODEL_DIR} \\\n",
    "    --output_directory {OUTPUT_DIR} \\\n",
    "    --pipeline_config_path {PIPELINE_CONFIG_PATH}\n",
    "\n",
    "print(\"âœ“ TFLite graph exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TFLite format\n",
    "import tensorflow as tf\n",
    "\n",
    "saved_model_dir = os.path.join(OUTPUT_DIR, 'saved_model')\n",
    "\n",
    "print(f\"Converting SavedModel to TFLite...\")\n",
    "print(f\"SavedModel directory: {saved_model_dir}\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(TFLITE_MODEL_PATH, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Get model size\n",
    "model_size_mb = os.path.getsize(TFLITE_MODEL_PATH) / (1024 * 1024)\n",
    "print(f\"âœ“ TFLite model saved to: {TFLITE_MODEL_PATH}\")\n",
    "print(f\"  Model size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify TFLite model\n",
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(\"âœ“ TFLite model loaded successfully\")\n",
    "print(\"\\nInput details:\")\n",
    "for detail in interpreter.get_input_details():\n",
    "    print(f\"  {detail}\")\n",
    "\n",
    "print(\"\\nOutput details:\")\n",
    "for detail in interpreter.get_output_details():\n",
    "    print(f\"  {detail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb606eb",
   "metadata": {},
   "source": [
    "## Create Quantized Model (INT8)\n",
    "\n",
    "Create an optimized INT8 quantized model for faster inference on edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf42469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create quantized INT8 model\n",
    "import glob\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "QUANT_MODEL_PATH = os.path.join(OUTPUT_DIR, 'model_quant.tflite')\n",
    "\n",
    "# Get training images for representative dataset\n",
    "train_images = glob.glob(os.path.join(TRAIN_DIR, '*.jpg')) + \\\n",
    "               glob.glob(os.path.join(TRAIN_DIR, '*.png'))\n",
    "\n",
    "print(f\"Using {len(train_images)} training images for quantization\")\n",
    "\n",
    "# Load original model to get input shape\n",
    "interpreter = tf.lite.Interpreter(TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "\n",
    "def representative_data_gen():\n",
    "    \"\"\"Generate representative dataset for quantization\"\"\"\n",
    "    num_samples = 300\n",
    "    for i in range(min(num_samples, len(train_images))):\n",
    "        image_path = random.choice(train_images)\n",
    "        image = tf.io.read_file(image_path)\n",
    "        \n",
    "        # Decode based on file extension\n",
    "        if image_path.endswith('.jpg') or image_path.endswith('.JPG'):\n",
    "            image = tf.io.decode_jpeg(image, channels=3)\n",
    "        elif image_path.endswith('.png'):\n",
    "            image = tf.io.decode_png(image, channels=3)\n",
    "        \n",
    "        image = tf.image.resize(image, [width, height])\n",
    "        image = tf.cast(image / 255., tf.float32)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        yield [image]\n",
    "\n",
    "print(\"Creating quantized model...\")\n",
    "\n",
    "# Initialize converter\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "# Enable full integer quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "# Convert\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save\n",
    "with open(QUANT_MODEL_PATH, 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "# Compare sizes\n",
    "float_size = os.path.getsize(TFLITE_MODEL_PATH) / (1024 * 1024)\n",
    "quant_size = os.path.getsize(QUANT_MODEL_PATH) / (1024 * 1024)\n",
    "reduction = (1 - quant_size/float_size) * 100\n",
    "\n",
    "print(f\"âœ“ Quantized model saved to: {QUANT_MODEL_PATH}\")\n",
    "print(f\"\\nModel Comparison:\")\n",
    "print(f\"  Float32 model: {float_size:.2f} MB\")\n",
    "print(f\"  INT8 model: {quant_size:.2f} MB\")\n",
    "print(f\"  Size reduction: {reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59857927",
   "metadata": {},
   "source": [
    "## Test Inference Performance\n",
    "\n",
    "Compare inference speed and accuracy between float32 and INT8 quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark models on GPU vs CPU\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Get test images\n",
    "test_images = glob.glob(os.path.join(TEST_DIR, '*.jpg'))[:20]\n",
    "\n",
    "def benchmark_model(model_path, images, device='/GPU:0'):\n",
    "    \"\"\"Benchmark model inference time\"\"\"\n",
    "    import cv2\n",
    "    \n",
    "    # For TFLite, we can't directly control device, but we can measure\n",
    "    interpreter = tf.lite.Interpreter(model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "    float_input = (input_details[0]['dtype'] == np.float32)\n",
    "    \n",
    "    times = []\n",
    "    confidences = []\n",
    "    \n",
    "    for img_path in images:\n",
    "        # Load and preprocess\n",
    "        image = cv2.imread(img_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_resized = cv2.resize(image_rgb, (width, height))\n",
    "        input_data = np.expand_dims(image_resized, axis=0)\n",
    "        \n",
    "        if float_input:\n",
    "            input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "        else:\n",
    "            input_data = np.uint8(input_data)\n",
    "        \n",
    "        # Time inference\n",
    "        start = time.perf_counter()\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        scores = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        times.append(end - start)\n",
    "        confidences.extend([s for s in scores if s > 0.5])\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "    avg_conf = np.mean(confidences) if confidences else 0\n",
    "    \n",
    "    return avg_time, avg_conf, len(confidences)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE BENCHMARK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Benchmark float32 model\n",
    "print(\"\\nBenchmarking Float32 model...\")\n",
    "float_time, float_conf, float_dets = benchmark_model(TFLITE_MODEL_PATH, test_images)\n",
    "\n",
    "# Benchmark INT8 model\n",
    "print(\"Benchmarking INT8 quantized model...\")\n",
    "quant_time, quant_conf, quant_dets = benchmark_model(QUANT_MODEL_PATH, test_images)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFloat32 Model:\")\n",
    "print(f\"  Average inference time: {float_time:.2f} ms\")\n",
    "print(f\"  Average confidence: {float_conf:.4f}\")\n",
    "print(f\"  Total detections: {float_dets}\")\n",
    "\n",
    "print(f\"\\nINT8 Quantized Model:\")\n",
    "print(f\"  Average inference time: {quant_time:.2f} ms\")\n",
    "print(f\"  Average confidence: {quant_conf:.4f}\")\n",
    "print(f\"  Total detections: {quant_dets}\")\n",
    "\n",
    "print(f\"\\nSpeedup: {float_time/quant_time:.2f}x faster\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f398de7",
   "metadata": {},
   "source": [
    "## Visualize Detection Results\n",
    "\n",
    "Test the model on sample images and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f79d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detections\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "def detect_and_visualize(model_path, image_paths, num_images=5, threshold=0.5):\n",
    "    \"\"\"Run detection and visualize results\"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    interpreter = Interpreter(model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "    float_input = (input_details[0]['dtype'] == np.float32)\n",
    "    \n",
    "    labels = ['background', 'HVAC_Spec_Label']\n",
    "    \n",
    "    # Select random images\n",
    "    selected_images = random.sample(image_paths, min(num_images, len(image_paths)))\n",
    "    \n",
    "    for img_path in selected_images:\n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        imH, imW, _ = image.shape\n",
    "        \n",
    "        # Preprocess\n",
    "        image_resized = cv2.resize(image_rgb, (width, height))\n",
    "        input_data = np.expand_dims(image_resized, axis=0)\n",
    "        \n",
    "        if float_input:\n",
    "            input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "        else:\n",
    "            input_data = np.uint8(input_data)\n",
    "        \n",
    "        # Inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        boxes = interpreter.get_tensor(output_details[1]['index'])[0]\n",
    "        classes = interpreter.get_tensor(output_details[3]['index'])[0]\n",
    "        scores = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "        \n",
    "        # Draw detections\n",
    "        for i in range(len(scores)):\n",
    "            if scores[i] > threshold and scores[i] <= 1.0:\n",
    "                ymin = int(max(1, boxes[i][0] * imH))\n",
    "                xmin = int(max(1, boxes[i][1] * imW))\n",
    "                ymax = int(min(imH, boxes[i][2] * imH))\n",
    "                xmax = int(min(imW, boxes[i][3] * imW))\n",
    "                \n",
    "                # Draw box\n",
    "                cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "                \n",
    "                # Draw label\n",
    "                label = f'{labels[int(classes[i])]}: {int(scores[i]*100)}%'\n",
    "                cv2.putText(image, label, (xmin, ymin-10),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Detection: {os.path.basename(img_path)}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize detections\n",
    "test_images = glob.glob(os.path.join(TEST_DIR, '*.jpg'))\n",
    "\n",
    "print(\"Visualizing Float32 model detections:\")\n",
    "detect_and_visualize(TFLITE_MODEL_PATH, test_images, num_images=5, threshold=0.5)\n",
    "\n",
    "print(\"\\nVisualizing INT8 quantized model detections:\")\n",
    "detect_and_visualize(QUANT_MODEL_PATH, test_images, num_images=5, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a84261",
   "metadata": {},
   "source": [
    "## Training Summary\n",
    "\n",
    "**Key Features:**\n",
    "- âœ“ GPU acceleration with CUDA\n",
    "- âœ“ Mixed precision training (float16/float32)\n",
    "- âœ“ XLA compilation for optimized operations\n",
    "- âœ“ Increased batch size for better GPU utilization\n",
    "- âœ“ TFLite export for deployment\n",
    "- âœ“ INT8 quantization for edge devices\n",
    "\n",
    "**Model Files:**\n",
    "- Float32 TFLite: `/workspace/exported_model/model.tflite`\n",
    "- INT8 TFLite: `/workspace/exported_model/model_quant.tflite`\n",
    "- Checkpoints: `/workspace/models_checkpoints/trained_model/`\n",
    "\n",
    "**Next Steps:**\n",
    "1. Deploy the TFLite model to your target device\n",
    "2. Fine-tune hyperparameters if needed\n",
    "3. Collect more training data to improve accuracy\n",
    "4. Experiment with different model architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
