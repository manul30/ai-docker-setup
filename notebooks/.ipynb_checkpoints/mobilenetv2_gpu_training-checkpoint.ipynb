{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6812f98b",
   "metadata": {},
   "source": [
    "# MobileNetV2 Object Detection Training - GPU Accelerated\n",
    "\n",
    "**Environment:**\n",
    "- TensorFlow 2.17.0 with GPU support\n",
    "- CUDA 12+ with cuDNN\n",
    "- RTX 5050 GPU with XLA acceleration\n",
    "- NVIDIA NGC TensorFlow Container\n",
    "\n",
    "This notebook trains a MobileNetV2-based SSD object detection model with GPU acceleration and XLA optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability and configuration\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "\n",
    "# List physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"\\nAvailable GPUs: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    print(f\"  - {gpu}\")\n",
    "\n",
    "if gpus:\n",
    "    # Enable memory growth to prevent TF from allocating all GPU memory at once\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"\\nâœ“ GPU memory growth enabled\")\n",
    "    \n",
    "    # Get GPU details\n",
    "    gpu_details = tf.config.experimental.get_device_details(gpus[0])\n",
    "    print(f\"\\nGPU Details: {gpu_details}\")\n",
    "    \n",
    "    # Enable XLA compilation for better performance\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print(\"âœ“ XLA compilation enabled\")\n",
    "else:\n",
    "    print(\"\\nâš  WARNING: No GPU detected! Training will use CPU.\")\n",
    "\n",
    "# Enable mixed precision for faster training on modern GPUs\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"âœ“ Mixed precision (float16) enabled for faster training\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff1cd40",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Install required packages for TensorFlow Object Detection API and dataset handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import sys\n",
    "\n",
    "# Uninstall Cython to avoid conflicts\n",
    "!pip uninstall Cython -y\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q roboflow protobuf==3.20.3 tensorflow-model-optimization\n",
    "\n",
    "print(\"âœ“ Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone TensorFlow models repository\n",
    "import os\n",
    "\n",
    "models_dir = '/workspace/models'\n",
    "if not os.path.exists(models_dir):\n",
    "    print(\"Cloning TensorFlow models repository...\")\n",
    "    !git clone --depth 1 https://github.com/tensorflow/models {models_dir}\n",
    "    print(\"âœ“ Repository cloned\")\n",
    "else:\n",
    "    print(\"âœ“ TensorFlow models repository already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Object Detection API\n",
    "import os\n",
    "\n",
    "research_dir = '/workspace/models/research'\n",
    "os.chdir(research_dir)\n",
    "\n",
    "# Compile protobuf files\n",
    "print(\"Compiling protobuf files...\")\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "\n",
    "# Copy setup.py\n",
    "!cp object_detection/packages/tf2/setup.py .\n",
    "\n",
    "print(\"âœ“ Object Detection API setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Object Detection API\n",
    "import os\n",
    "\n",
    "os.chdir('/workspace/models/research')\n",
    "\n",
    "# Install the API\n",
    "print(\"Installing Object Detection API...\")\n",
    "!python -m pip install . --quiet\n",
    "\n",
    "print(\"âœ“ Object Detection API installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Object Detection API installation\n",
    "print(\"Testing Object Detection API installation...\")\n",
    "os.chdir('/workspace/models/research')\n",
    "!python object_detection/builders/model_builder_tf2_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a1b44b",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "Download dataset using Roboflow and prepare TFRecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab06f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Roboflow\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Initialize Roboflow with your API key\n",
    "rf = Roboflow(api_key=\"LHiJvoAFmvmbSi50SwC1\")\n",
    "project = rf.workspace(\"hvac-whaik\").project(\"ai-hvac-nameplate-focus-kcnb5\")\n",
    "version = project.version(12)\n",
    "\n",
    "# Download dataset to workspace\n",
    "dataset = version.download(\"tensorflow\", location=\"/workspace/dataset\")\n",
    "print(f\"âœ“ Dataset downloaded to: {dataset.location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ce622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset paths and create label map\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import dataset_util\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Dataset paths\n",
    "DATASET_DIR = \"/workspace/dataset/ai-hvac-nameplate-focus-12\"\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATASET_DIR, 'test')\n",
    "VALID_DIR = os.path.join(DATASET_DIR, 'valid')\n",
    "\n",
    "# Label map\n",
    "label_map = {\n",
    "    1: 'HVAC_Spec_Label'\n",
    "}\n",
    "\n",
    "def create_label_map_file(label_map, output_path):\n",
    "    \"\"\"Create label map file in pbtxt format\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        for id, name in label_map.items():\n",
    "            f.write(f'item {{\\n  id: {id}\\n  name: \"{name}\"\\n}}\\n')\n",
    "\n",
    "LABEL_MAP_PATH = os.path.join(DATASET_DIR, 'label_map.pbtxt')\n",
    "create_label_map_file(label_map, LABEL_MAP_PATH)\n",
    "\n",
    "print(\"âœ“ Label map created\")\n",
    "print(f\"  Classes: {list(label_map.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc449078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CSV annotations to TFRecord format\n",
    "def create_tf_example(row, image_dir):\n",
    "    \"\"\"Convert a single annotation row to TFRecord example\"\"\"\n",
    "    filename = row['filename']\n",
    "    img_path = os.path.join(image_dir, filename)\n",
    "    \n",
    "    with tf.io.gfile.GFile(img_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    \n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename_bytes = filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    \n",
    "    xmins = [row['xmin'] / width]\n",
    "    xmaxs = [row['xmax'] / width]\n",
    "    ymins = [row['ymin'] / height]\n",
    "    ymaxs = [row['ymax'] / height]\n",
    "    classes_text = [row['class'].encode('utf8')]\n",
    "    classes = [list(label_map.keys())[list(label_map.values()).index(row['class'])]]\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename_bytes),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename_bytes),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "def csv_to_tfrecord(csv_path, image_dir, output_path):\n",
    "    \"\"\"Convert CSV annotations to TFRecord file\"\"\"\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"Converting {len(df)} annotations from {os.path.basename(csv_path)}...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        tf_example = create_tf_example(row, image_dir)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    \n",
    "    writer.close()\n",
    "    print(f\"âœ“ Created {output_path}\")\n",
    "\n",
    "# Create TFRecords for train, test, and validation sets\n",
    "print(\"\\nCreating TFRecord files...\")\n",
    "\n",
    "TRAIN_CSV = os.path.join(TRAIN_DIR, '_annotations.csv')\n",
    "TEST_CSV = os.path.join(TEST_DIR, '_annotations.csv')\n",
    "VALID_CSV = os.path.join(VALID_DIR, '_annotations.csv')\n",
    "\n",
    "TRAIN_TFRECORD = os.path.join(DATASET_DIR, 'train.tfrecord')\n",
    "TEST_TFRECORD = os.path.join(DATASET_DIR, 'test.tfrecord')\n",
    "VALID_TFRECORD = os.path.join(DATASET_DIR, 'valid.tfrecord')\n",
    "\n",
    "csv_to_tfrecord(TRAIN_CSV, TRAIN_DIR, TRAIN_TFRECORD)\n",
    "csv_to_tfrecord(TEST_CSV, TEST_DIR, TEST_TFRECORD)\n",
    "csv_to_tfrecord(VALID_CSV, VALID_DIR, VALID_TFRECORD)\n",
    "\n",
    "print(\"\\nâœ“ All TFRecord files created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ab6e1",
   "metadata": {},
   "source": [
    "## Download Pre-trained Model\n",
    "\n",
    "Download MobileNetV2 SSD FPNLite pre-trained on COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ef2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained model\n",
    "import os\n",
    "\n",
    "MODEL_DIR = '/workspace/models_checkpoints'\n",
    "MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "MODEL_URL = f'http://download.tensorflow.org/models/object_detection/tf2/20200711/{MODEL_NAME}.tar.gz'\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Download and extract\n",
    "print(f\"Downloading {MODEL_NAME}...\")\n",
    "!wget -q {MODEL_URL} -O /tmp/{MODEL_NAME}.tar.gz\n",
    "!tar -xf /tmp/{MODEL_NAME}.tar.gz -C {MODEL_DIR}\n",
    "!rm /tmp/{MODEL_NAME}.tar.gz\n",
    "\n",
    "print(f\"âœ“ Model downloaded to {MODEL_DIR}/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e058d",
   "metadata": {},
   "source": [
    "## Configure Training Pipeline\n",
    "\n",
    "Update the pipeline configuration with dataset paths and GPU-optimized settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeeee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline for GPU training\n",
    "import re\n",
    "\n",
    "PIPELINE_CONFIG_PATH = os.path.join(MODEL_DIR, MODEL_NAME, 'pipeline.config')\n",
    "\n",
    "# Read the pipeline config\n",
    "with open(PIPELINE_CONFIG_PATH, 'r') as f:\n",
    "    config = f.read()\n",
    "\n",
    "# Checkpoint path (empty string to train from scratch, or use pre-trained weights)\n",
    "checkpoint_path = os.path.join(MODEL_DIR, MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
    "\n",
    "# Replace placeholders with actual paths\n",
    "config = re.sub(r'num_classes: \\d+', 'num_classes: 1', config)\n",
    "config = re.sub(r'batch_size: \\d+', 'batch_size: 16', config)  # Increased batch size for GPU\n",
    "config = re.sub(r'fine_tune_checkpoint: \".*?\"', f'fine_tune_checkpoint: \"{checkpoint_path}\"', config)\n",
    "config = re.sub(r'fine_tune_checkpoint_type: \".*?\"', 'fine_tune_checkpoint_type: \"detection\"', config)\n",
    "\n",
    "# Update train config\n",
    "config = re.sub(r'label_map_path: \".*?\"', f'label_map_path: \"{LABEL_MAP_PATH}\"', config, count=1)\n",
    "config = re.sub(r'input_path: \".*?\"', f'input_path: \"{TRAIN_TFRECORD}\"', config, count=1)\n",
    "\n",
    "# Update eval config\n",
    "config = re.sub(r'label_map_path: \".*?\"', f'label_map_path: \"{LABEL_MAP_PATH}\"', config, count=1)\n",
    "config = re.sub(r'input_path: \".*?\"', f'input_path: \"{VALID_TFRECORD}\"', config, count=1)\n",
    "\n",
    "# Add GPU-specific optimizations\n",
    "if 'use_bfloat16: false' in config:\n",
    "    config = config.replace('use_bfloat16: false', 'use_bfloat16: true')  # Enable bfloat16 for faster training\n",
    "\n",
    "# Write back the modified config\n",
    "with open(PIPELINE_CONFIG_PATH, 'w') as f:\n",
    "    f.write(config)\n",
    "\n",
    "print(\"âœ“ Pipeline configuration updated for GPU training\")\n",
    "print(f\"  - Batch size: 16 (optimized for GPU)\")\n",
    "print(f\"  - Mixed precision: Enabled\")\n",
    "print(f\"  - Fine-tune checkpoint: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba87a19",
   "metadata": {},
   "source": [
    "## Train the Model ðŸš€\n",
    "\n",
    "Train with GPU acceleration and XLA optimization. The training will automatically use:\n",
    "- GPU acceleration\n",
    "- Mixed precision (float16/float32)\n",
    "- XLA compilation for optimized operations\n",
    "- Increased batch size for better GPU utilization\n",
    "\n",
    "**Note:** Training will save checkpoints to `/workspace/models_checkpoints/trained_model` every few minutes. You can resume training by running this cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab9a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with GPU acceleration\n",
    "import os\n",
    "\n",
    "# Output directory for trained model\n",
    "TRAINED_MODEL_DIR = os.path.join(MODEL_DIR, 'trained_model')\n",
    "os.makedirs(TRAINED_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Training parameters\n",
    "NUM_TRAIN_STEPS = 15000  # Adjust based on your needs\n",
    "CHECKPOINT_EVERY_N = 500  # Save checkpoint every N steps\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING GPU TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Pipeline config: {PIPELINE_CONFIG_PATH}\")\n",
    "print(f\"Model output dir: {TRAINED_MODEL_DIR}\")\n",
    "print(f\"Training steps: {NUM_TRAIN_STEPS}\")\n",
    "print(f\"GPU: {gpus[0] if gpus else 'CPU (No GPU detected)'}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Change to research directory\n",
    "os.chdir('/workspace/models/research')\n",
    "\n",
    "# Run training with GPU\n",
    "# The model_main_tf2.py script will automatically use GPU if available\n",
    "!python object_detection/model_main_tf2.py \\\n",
    "    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n",
    "    --model_dir={TRAINED_MODEL_DIR} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={NUM_TRAIN_STEPS} \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --checkpoint_every_n={CHECKPOINT_EVERY_N}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c56e0d",
   "metadata": {},
   "source": [
    "## Monitor Training with TensorBoard\n",
    "\n",
    "Launch TensorBoard to monitor training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {TRAINED_MODEL_DIR} --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9813d0",
   "metadata": {},
   "source": [
    "## Export Trained Model\n",
    "\n",
    "Export the trained model to SavedModel format and convert to TFLite for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to TFLite format\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = '/workspace/exported_model'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TFLITE_MODEL_PATH = os.path.join(OUTPUT_DIR, 'model.tflite')\n",
    "\n",
    "print(\"Exporting model for TFLite...\")\n",
    "os.chdir('/workspace/models/research')\n",
    "\n",
    "# Export TFLite graph\n",
    "!python object_detection/export_tflite_graph_tf2.py \\\n",
    "    --trained_checkpoint_dir {TRAINED_MODEL_DIR} \\\n",
    "    --output_directory {OUTPUT_DIR} \\\n",
    "    --pipeline_config_path {PIPELINE_CONFIG_PATH}\n",
    "\n",
    "print(\"âœ“ TFLite graph exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TFLite format\n",
    "import tensorflow as tf\n",
    "\n",
    "saved_model_dir = os.path.join(OUTPUT_DIR, 'saved_model')\n",
    "\n",
    "print(f\"Converting SavedModel to TFLite...\")\n",
    "print(f\"SavedModel directory: {saved_model_dir}\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(TFLITE_MODEL_PATH, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Get model size\n",
    "model_size_mb = os.path.getsize(TFLITE_MODEL_PATH) / (1024 * 1024)\n",
    "print(f\"âœ“ TFLite model saved to: {TFLITE_MODEL_PATH}\")\n",
    "print(f\"  Model size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify TFLite model\n",
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(\"âœ“ TFLite model loaded successfully\")\n",
    "print(\"\\nInput details:\")\n",
    "for detail in interpreter.get_input_details():\n",
    "    print(f\"  {detail}\")\n",
    "\n",
    "print(\"\\nOutput details:\")\n",
    "for detail in interpreter.get_output_details():\n",
    "    print(f\"  {detail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb606eb",
   "metadata": {},
   "source": [
    "## Create Quantized Model (INT8)\n",
    "\n",
    "Create an optimized INT8 quantized model for faster inference on edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf42469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create quantized INT8 model\n",
    "import glob\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "QUANT_MODEL_PATH = os.path.join(OUTPUT_DIR, 'model_quant.tflite')\n",
    "\n",
    "# Get training images for representative dataset\n",
    "train_images = glob.glob(os.path.join(TRAIN_DIR, '*.jpg')) + \\\n",
    "               glob.glob(os.path.join(TRAIN_DIR, '*.png'))\n",
    "\n",
    "print(f\"Using {len(train_images)} training images for quantization\")\n",
    "\n",
    "# Load original model to get input shape\n",
    "interpreter = tf.lite.Interpreter(TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "\n",
    "def representative_data_gen():\n",
    "    \"\"\"Generate representative dataset for quantization\"\"\"\n",
    "    num_samples = 300\n",
    "    for i in range(min(num_samples, len(train_images))):\n",
    "        image_path = random.choice(train_images)\n",
    "        image = tf.io.read_file(image_path)\n",
    "        \n",
    "        # Decode based on file extension\n",
    "        if image_path.endswith('.jpg') or image_path.endswith('.JPG'):\n",
    "            image = tf.io.decode_jpeg(image, channels=3)\n",
    "        elif image_path.endswith('.png'):\n",
    "            image = tf.io.decode_png(image, channels=3)\n",
    "        \n",
    "        image = tf.image.resize(image, [width, height])\n",
    "        image = tf.cast(image / 255., tf.float32)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        yield [image]\n",
    "\n",
    "print(\"Creating quantized model...\")\n",
    "\n",
    "# Initialize converter\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "# Enable full integer quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "# Convert\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save\n",
    "with open(QUANT_MODEL_PATH, 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "# Compare sizes\n",
    "float_size = os.path.getsize(TFLITE_MODEL_PATH) / (1024 * 1024)\n",
    "quant_size = os.path.getsize(QUANT_MODEL_PATH) / (1024 * 1024)\n",
    "reduction = (1 - quant_size/float_size) * 100\n",
    "\n",
    "print(f\"âœ“ Quantized model saved to: {QUANT_MODEL_PATH}\")\n",
    "print(f\"\\nModel Comparison:\")\n",
    "print(f\"  Float32 model: {float_size:.2f} MB\")\n",
    "print(f\"  INT8 model: {quant_size:.2f} MB\")\n",
    "print(f\"  Size reduction: {reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59857927",
   "metadata": {},
   "source": [
    "## Test Inference Performance\n",
    "\n",
    "Compare inference speed and accuracy between float32 and INT8 quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark models on GPU vs CPU\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Get test images\n",
    "test_images = glob.glob(os.path.join(TEST_DIR, '*.jpg'))[:20]\n",
    "\n",
    "def benchmark_model(model_path, images, device='/GPU:0'):\n",
    "    \"\"\"Benchmark model inference time\"\"\"\n",
    "    import cv2\n",
    "    \n",
    "    # For TFLite, we can't directly control device, but we can measure\n",
    "    interpreter = tf.lite.Interpreter(model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "    float_input = (input_details[0]['dtype'] == np.float32)\n",
    "    \n",
    "    times = []\n",
    "    confidences = []\n",
    "    \n",
    "    for img_path in images:\n",
    "        # Load and preprocess\n",
    "        image = cv2.imread(img_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_resized = cv2.resize(image_rgb, (width, height))\n",
    "        input_data = np.expand_dims(image_resized, axis=0)\n",
    "        \n",
    "        if float_input:\n",
    "            input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "        else:\n",
    "            input_data = np.uint8(input_data)\n",
    "        \n",
    "        # Time inference\n",
    "        start = time.perf_counter()\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        scores = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        times.append(end - start)\n",
    "        confidences.extend([s for s in scores if s > 0.5])\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "    avg_conf = np.mean(confidences) if confidences else 0\n",
    "    \n",
    "    return avg_time, avg_conf, len(confidences)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE BENCHMARK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Benchmark float32 model\n",
    "print(\"\\nBenchmarking Float32 model...\")\n",
    "float_time, float_conf, float_dets = benchmark_model(TFLITE_MODEL_PATH, test_images)\n",
    "\n",
    "# Benchmark INT8 model\n",
    "print(\"Benchmarking INT8 quantized model...\")\n",
    "quant_time, quant_conf, quant_dets = benchmark_model(QUANT_MODEL_PATH, test_images)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFloat32 Model:\")\n",
    "print(f\"  Average inference time: {float_time:.2f} ms\")\n",
    "print(f\"  Average confidence: {float_conf:.4f}\")\n",
    "print(f\"  Total detections: {float_dets}\")\n",
    "\n",
    "print(f\"\\nINT8 Quantized Model:\")\n",
    "print(f\"  Average inference time: {quant_time:.2f} ms\")\n",
    "print(f\"  Average confidence: {quant_conf:.4f}\")\n",
    "print(f\"  Total detections: {quant_dets}\")\n",
    "\n",
    "print(f\"\\nSpeedup: {float_time/quant_time:.2f}x faster\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f398de7",
   "metadata": {},
   "source": [
    "## Visualize Detection Results\n",
    "\n",
    "Test the model on sample images and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f79d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detections\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "def detect_and_visualize(model_path, image_paths, num_images=5, threshold=0.5):\n",
    "    \"\"\"Run detection and visualize results\"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    interpreter = Interpreter(model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "    float_input = (input_details[0]['dtype'] == np.float32)\n",
    "    \n",
    "    labels = ['background', 'HVAC_Spec_Label']\n",
    "    \n",
    "    # Select random images\n",
    "    selected_images = random.sample(image_paths, min(num_images, len(image_paths)))\n",
    "    \n",
    "    for img_path in selected_images:\n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        imH, imW, _ = image.shape\n",
    "        \n",
    "        # Preprocess\n",
    "        image_resized = cv2.resize(image_rgb, (width, height))\n",
    "        input_data = np.expand_dims(image_resized, axis=0)\n",
    "        \n",
    "        if float_input:\n",
    "            input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "        else:\n",
    "            input_data = np.uint8(input_data)\n",
    "        \n",
    "        # Inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        boxes = interpreter.get_tensor(output_details[1]['index'])[0]\n",
    "        classes = interpreter.get_tensor(output_details[3]['index'])[0]\n",
    "        scores = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "        \n",
    "        # Draw detections\n",
    "        for i in range(len(scores)):\n",
    "            if scores[i] > threshold and scores[i] <= 1.0:\n",
    "                ymin = int(max(1, boxes[i][0] * imH))\n",
    "                xmin = int(max(1, boxes[i][1] * imW))\n",
    "                ymax = int(min(imH, boxes[i][2] * imH))\n",
    "                xmax = int(min(imW, boxes[i][3] * imW))\n",
    "                \n",
    "                # Draw box\n",
    "                cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "                \n",
    "                # Draw label\n",
    "                label = f'{labels[int(classes[i])]}: {int(scores[i]*100)}%'\n",
    "                cv2.putText(image, label, (xmin, ymin-10),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Detection: {os.path.basename(img_path)}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize detections\n",
    "test_images = glob.glob(os.path.join(TEST_DIR, '*.jpg'))\n",
    "\n",
    "print(\"Visualizing Float32 model detections:\")\n",
    "detect_and_visualize(TFLITE_MODEL_PATH, test_images, num_images=5, threshold=0.5)\n",
    "\n",
    "print(\"\\nVisualizing INT8 quantized model detections:\")\n",
    "detect_and_visualize(QUANT_MODEL_PATH, test_images, num_images=5, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a84261",
   "metadata": {},
   "source": [
    "## Training Summary\n",
    "\n",
    "**Key Features:**\n",
    "- âœ“ GPU acceleration with CUDA\n",
    "- âœ“ Mixed precision training (float16/float32)\n",
    "- âœ“ XLA compilation for optimized operations\n",
    "- âœ“ Increased batch size for better GPU utilization\n",
    "- âœ“ TFLite export for deployment\n",
    "- âœ“ INT8 quantization for edge devices\n",
    "\n",
    "**Model Files:**\n",
    "- Float32 TFLite: `/workspace/exported_model/model.tflite`\n",
    "- INT8 TFLite: `/workspace/exported_model/model_quant.tflite`\n",
    "- Checkpoints: `/workspace/models_checkpoints/trained_model/`\n",
    "\n",
    "**Next Steps:**\n",
    "1. Deploy the TFLite model to your target device\n",
    "2. Fine-tune hyperparameters if needed\n",
    "3. Collect more training data to improve accuracy\n",
    "4. Experiment with different model architectures"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
